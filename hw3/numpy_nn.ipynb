{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numpy_nn_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEXCzL7-1IHo"
      },
      "source": [
        "NN_ARCHITECTURE = [\n",
        "    {\"input_dim\": 3, \"output_dim\": 25, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 25, \"output_dim\": 2, \"activation\": \"sigmoid\"},\n",
        "]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYwUenRsbRb6"
      },
      "source": [
        "def init_layers(nn_architecture, seed = 99):\n",
        "    # random seed initiation\n",
        "    np.random.seed(seed)\n",
        "    # number of layers in our neural network\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    # parameters storage initiation\n",
        "    params_values = {}\n",
        "    \n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        # we number network layers from 1\n",
        "        layer_idx = idx + 1\n",
        "        \n",
        "        # extracting the number of units in layers\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "        \n",
        "        # initiating the values of the W matrix\n",
        "        # and vector b for subsequent layers\n",
        "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, layer_input_size) * 0.1\n",
        "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, 1) * 0.1\n",
        "        \n",
        "    return params_values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yjc19Z91OG2"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0;\n",
        "    return dZ;"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q13ir081SNs"
      },
      "source": [
        "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
        "    # calculation of the input value for the activation function\n",
        "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
        "    \n",
        "    # selection of activation function\n",
        "    if activation is \"relu\":\n",
        "        activation_func = relu\n",
        "    elif activation is \"sigmoid\":\n",
        "        activation_func = sigmoid\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "        \n",
        "    # return of calculated activation A and the intermediate Z matrix\n",
        "    return activation_func(Z_curr), Z_curr"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<ipython-input-4-d3e2bf7974c7>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if activation is \"relu\":\n<ipython-input-4-d3e2bf7974c7>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  elif activation is \"sigmoid\":\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SpIhVd-1ZVb"
      },
      "source": [
        "def full_forward_propagation(X, params_values, nn_architecture):\n",
        "    # creating a temporary memory to store the information needed for a backward step\n",
        "    memory = {}\n",
        "    # X vector is the activation for layer 0â€Š\n",
        "    A_curr = X\n",
        "    \n",
        "    # iteration over network layers\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        # we number network layers from 1\n",
        "        layer_idx = idx + 1\n",
        "        # transfer the activation from the previous iteration\n",
        "        A_prev = A_curr\n",
        "        \n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        # extraction of W for the current layer\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        # extraction of b for the current layer\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        # calculation of activation for the current layer\n",
        "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
        "        \n",
        "        # saving calculated values in the memory\n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "       \n",
        "    # return of prediction vector and a dictionary containing intermediate values\n",
        "    return A_curr, memory"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46y_QkpP1dQ4"
      },
      "source": [
        "def get_cost_value(Y_hat, Y):\n",
        "    # number of examples\n",
        "    m = Y_hat.shape[1]\n",
        "    # calculation of the cost according to the formula\n",
        "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "# an auxiliary function that converts probability into class\n",
        "def convert_prob_into_class(probs):\n",
        "    probs_ = np.copy(probs)\n",
        "    probs_[probs_ > 0.5] = 1\n",
        "    probs_[probs_ <= 0.5] = 0\n",
        "    return probs_\n",
        "\n",
        "def get_accuracy_value(Y_hat, Y):\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xTmYoEf1jKW"
      },
      "source": [
        "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
        "    # number of examples\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    # selection of activation function\n",
        "    if activation is \"relu\":\n",
        "        backward_activation_func = relu_backward\n",
        "    elif activation is \"sigmoid\":\n",
        "        backward_activation_func = sigmoid_backward\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "    \n",
        "    # calculation of the activation function derivative\n",
        "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "    \n",
        "    # derivative of the matrix W\n",
        "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
        "    # derivative of the vector b\n",
        "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
        "    # derivative of the matrix A_prev\n",
        "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "    return dA_prev, dW_curr, db_curr"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<ipython-input-7-0a7b214a3e1f>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if activation is \"relu\":\n<ipython-input-7-0a7b214a3e1f>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  elif activation is \"sigmoid\":\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUvr2Y_01oCH"
      },
      "source": [
        "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
        "    grads_values = {}\n",
        "    \n",
        "    # number of examples\n",
        "    m = Y.shape[1]\n",
        "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "    \n",
        "    # initiation of gradient descent algorithm\n",
        "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        # we number network layers from 1\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        # extraction of the activation function for the current layer\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        dA_curr = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "        \n",
        "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
        "        \n",
        "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
        "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
        "        \n",
        "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "    \n",
        "    return grads_values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OSlBTxI1sax"
      },
      "source": [
        "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
        "\n",
        "    # iteration over network layers\n",
        "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
        "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
        "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
        "\n",
        "    return params_values;"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj2_2VHI1v2w"
      },
      "source": [
        "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n",
        "    # initiation of neural net parameters\n",
        "    params_values = init_layers(nn_architecture, 2)\n",
        "    # initiation of lists storing the history \n",
        "    # of metrics calculated during the learning process \n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "    \n",
        "    # performing calculations for subsequent iterations\n",
        "    for i in range(epochs):\n",
        "        # step forward\n",
        "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
        "        \n",
        "        # calculating metrics and saving them in history\n",
        "        cost = get_cost_value(Y_hat, Y)\n",
        "        cost_history.append(cost)\n",
        "        accuracy = get_accuracy_value(Y_hat, Y)\n",
        "        accuracy_history.append(accuracy)\n",
        "        \n",
        "        # step backward - calculating gradient\n",
        "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
        "        # updating model state\n",
        "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
        "        \n",
        "        if(i % 50 == 0):\n",
        "            if(verbose):\n",
        "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
        "            if(callback is not None):\n",
        "                callback(i, params_values)\n",
        "            \n",
        "    return params_values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRlPaVGi13BY"
      },
      "source": [
        "# number of samples in the data set\n",
        "N_SAMPLES = 1000\n",
        "# ratio between training and test sets\n",
        "TEST_SIZE = 0.1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duBod3y52HG-"
      },
      "source": [
        "import os\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "#import keras\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.utils import np_utils\n",
        "#from keras import regularizers\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOXersNX1890"
      },
      "source": [
        "\n",
        "np.random.choice([1, 9, 20], 1000)\n",
        "\n",
        "d = {'col1': np.random.choice(range(1,1000,1), 1000), \n",
        "     'col2': np.random.choice(range(1,1000,1), 1000),\n",
        "     'col3': np.random.choice(range(1,100,1), 1000),\n",
        "     }\n",
        "\n",
        "d['col4'] = d['col1'] + d['col2'] + d['col3'] + np.random.choice([-1,1], 1)\n",
        "\n",
        "def calc_col5(x):\n",
        "  return 0 if x < 1500 else 1\n",
        "\n",
        "df = pd.DataFrame(data=d)\n",
        "df['col5'] = df['col4'].apply(calc_col5)\n",
        "\n",
        "#X, y = df.drop(['col4','col5'], axis=1).to_numpy(), df['col5'].values\n",
        "X, y = df.drop(['col4','col5'], axis=1).to_numpy(), df[['col4', 'col5']].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMLd1haV2VRv"
      },
      "source": [
        "def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n",
        "    if (dark):\n",
        "        plt.style.use('dark_background')\n",
        "    else:\n",
        "        sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(16,12))\n",
        "    axes = plt.gca()\n",
        "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
        "    plt.title(plot_name, fontsize=30)\n",
        "    plt.subplots_adjust(left=0.20)\n",
        "    plt.subplots_adjust(right=0.80)\n",
        "    if(XX is not None and YY is not None and preds is not None):\n",
        "        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n",
        "        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n",
        "    if(file_name):\n",
        "        plt.savefig(file_name)\n",
        "        plt.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "tjfHIKuz2X6A",
        "outputId": "eae33c03-9588-4dad-b0cd-d44429816b31"
      },
      "source": [
        "#make_plot(X, y, \"Dataset\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzLRQdRi2dHs",
        "outputId": "f31bdea3-0e2f-41ef-dd22-f44f5fd034e1"
      },
      "source": [
        "# Training\n",
        "#params_values = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 10000, 0.01)\n",
        "params_values = train(np.transpose(X_train), y_train.T, NN_ARCHITECTURE, 10000, 0.01)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b6e01dbfb102>:5: RuntimeWarning: divide by zero encountered in log\n  cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n<ipython-input-8-3740a207dd88>:10: RuntimeWarning: divide by zero encountered in true_divide\n  dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n<ipython-input-8-3740a207dd88>:10: RuntimeWarning: invalid value encountered in true_divide\n  dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n<ipython-input-3-05b869355ad7>:9: RuntimeWarning: invalid value encountered in multiply\n  return dA * sig * (1 - sig)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BInk-2RgtbR2"
      },
      "source": [
        "# Prediction\n",
        "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYt1TCCTtcJ9",
        "outputId": "a8ca10d5-542e-432b-9c64-3420326e29bf"
      },
      "source": [
        "# Accuracy achieved on the test set\n",
        "acc_test = get_accuracy_value(Y_test_hat, y_test.T)#np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
        "print(\"Test set accuracy: {:.2f} - David\".format(acc_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.00 - David\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15kb0y5XT3Py",
        "outputId": "5964aa88-23b0-47d6-c0eb-7ec5b3f0db53"
      },
      "source": [
        "Y_test_hat"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "       [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}